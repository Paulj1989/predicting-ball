name: Daily Model Update

on:
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      force_tune:
        description: 'Force hyperparameter tuning'
        type: boolean
        default: false

# cancel previous runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  update-model:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: read
      issues: write
    outputs:
      tune_status: ${{ steps.check_tuning.outputs.tune }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Install dependencies
        run: uv pip install --system -e .

      - name: Download DuckDB
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACE_NAME: ${{ secrets.DO_SPACE_NAME }}
          DO_SPACE_REGION: ${{ secrets.DO_SPACE_REGION }}
        run: python scripts/automation/download_db.py

      - name: Check tuning cache
        id: cache-tuning
        uses: actions/cache@v4
        with:
          path: tuning_marker.txt
          # uses a unique key per run but restores the most recent version
          key: tuning-timestamp-${{ github.run_id }}
          restore-keys: tuning-timestamp-

      - name: Determine tuning status
        id: check_tuning
        run: |
          NOW=$(date +%s)
          TUNE_NEEDED="true"

          # check if 30 days have passed since the last cached timestamp
          if [ -f "tuning_marker.txt" ]; then
            LAST_TUNE=$(cat tuning_marker.txt)
            if [ $(( NOW - LAST_TUNE )) -lt 2592000 ]; then
              TUNE_NEEDED="false"
            fi
          fi

          if [ "${{ inputs.force_tune }}" = "true" ]; then
            TUNE_NEEDED="true"
          fi

          echo "tune=$TUNE_NEEDED" >> $GITHUB_OUTPUT
          # update the marker only if a tune is actually being performed
          [ "$TUNE_NEEDED" = "true" ] && echo "$NOW" > tuning_marker.txt
          echo "Tuning: $TUNE_NEEDED"

      - name: Download previous model
        if: steps.check_tuning.outputs.tune != 'true'
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACE_NAME: ${{ secrets.DO_SPACE_NAME }}
          DO_SPACE_REGION: ${{ secrets.DO_SPACE_REGION }}
        run: python scripts/automation/download_db.py --model-only

      - name: Train model
        run: |
          mkdir -p outputs/models
          CMD="python scripts/modeling/train_model.py"
          [ "${{ steps.check_tuning.outputs.tune }}" = "true" ] && CMD="$CMD --tune"
          $CMD

      - name: Calibrate and validate
        run: |
          python scripts/modeling/run_calibration.py \
            --model-path outputs/models/production_model.pkl \
            --comprehensive \
            --outcome-specific
          python scripts/modeling/validate_model.py \
            --model-path outputs/models/production_model.pkl \
            --calibrator-path outputs/models/calibrators.pkl \
            --output-json outputs/validation/metrics.json

      - name: Generate predictions and upload
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACE_NAME: ${{ secrets.DO_SPACE_NAME }}
          DO_SPACE_REGION: ${{ secrets.DO_SPACE_REGION }}
        run: |
          python scripts/modeling/generate_predictions.py \
            --model-path outputs/models/production_model.pkl \
            --calibrator-path outputs/models/calibrators.pkl \
            --validation-metrics outputs/validation/metrics.json \
            --n-bootstrap 250 \
            --n-simulations 10000

      - name: Notify training failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Model Update Failed: ${new Date().toLocaleDateString()}`,
              body: `Update failed at the training stage. [View Logs](${runUrl})`,
              labels: ['automation', 'bug']
            });

  deploy:
    needs: update-model
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
      deployments: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_ACCESS_TOKEN }}

      - name: Trigger deployment
        id: deploy
        uses: digitalocean/app_action/deploy@v2
        with:
          token: ${{ secrets.DO_ACCESS_TOKEN }}

      - name: Verify deployment
        run: |
          # extract metadata from the deployment step to monitor progress
          APP_ID=$(echo '${{ steps.deploy.outputs.app }}' | jq -r '.id')
          DEPLOY_ID=$(echo '${{ steps.deploy.outputs.app }}' | jq -r '.pending_deployment.id // .active_deployment.id')

          echo "Monitoring App $APP_ID, Deployment $DEPLOY_ID"

          # poll the DigitalOcean API until the app is active or has failed
          while true; do
            PHASE=$(doctl apps get-deployment $APP_ID $DEPLOY_ID --format Phase --no-header)
            echo "Phase: $PHASE"
            [ "$PHASE" = "ACTIVE" ] && exit 0
            if [[ "$PHASE" =~ ^(ERROR|CANCELED|FAILED)$ ]]; then
              echo "Deployment failed."
              exit 1
            fi
            sleep 30
          done

      - name: Notify deployment failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Deployment Failed: ${new Date().toLocaleDateString()}`,
              body: `Training succeeded, but deployment failed. [View Logs](${runUrl})`,
              labels: ['automation', 'deployment']
            });

  report:
    needs: [update-model, deploy]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Create job summary
        env:
          URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          echo "## Pipeline Summary: $(date +'%Y-%m-%d')" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Result |" >> $GITHUB_STEP_SUMMARY
          echo "| :--- | :--- |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Update | ${{ needs.update-model.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| App Deployment | ${{ needs.deploy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Tuning**: ${{ needs.update-model.outputs.tune_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow Link**: [View Run]($URL)" >> $GITHUB_STEP_SUMMARY
